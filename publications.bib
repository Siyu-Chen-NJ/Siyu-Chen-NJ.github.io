
@inproceedings{lee_lightweight_2022,
	title = {Lightweight {Neural} {Architecture} {Search} with {Parameter} {Remapping} and {Knowledge} {Distillation}},
	copyright = {CC0 1.0 Universal Public Domain Dedication},
	url = {https://openreview.net/forum?id=3D2Qz9y001S},
	abstract = {Designing diverse neural architectures taking into account resource constraints or datasets is one of the main challenges in Neural Architecture Search (NAS). However, existing sample-based or one-shot NAS approaches require excessive time or computational cost to be used in multiple practical scenarios. Recently, to alleviate such issues, zero-cost NAS methods that are efficient proxies have been proposed, yet their performance is rather poor due to the strong assumption that they predict the final performance of a given architecture with random initialization. In this work, we propose a novel NAS based on block-wise parameter remapping (PR) and knowledge distillation (KD), which shows high predictive performance while being fast and lightweight enough to be used iteratively to support multiple real-world scenarios. PR significantly shortens training steps and accordingly we can reduce the required time/data for KD to work as an accurate proxy to just few batches, which is largely practical in real-world. In the experiments, we validate the proposed method for its accuracy estimation performance on CIFAR-10 from the MobileNetV3 search space. It outperforms all relevant baselines in terms of performance estimation with only 20 batches.},
	language = {en},
	urldate = {2024-02-27},
	author = {Lee, Hayeon and An, Sohyun and Kim, Minseon and Hwang, Sung Ju},
	month = may,
	year = {2022},
	file = {Full Text PDF:/Users/ansohyeon/Zotero/storage/MWNZMU6I/Lee 등 - 2022 - Lightweight Neural Architecture Search with Parame.pdf:application/pdf},
}

@article{lee2023meta,
  title={Meta-prediction Model for Distillation-Aware NAS on Unseen Datasets},
  author={Lee, Hayeon and An, Sohyun and Kim, Minseon and Hwang, Sung Ju},
  journal={arXiv preprint arXiv:2305.16948},
  year={2023}
}

@inproceedings{an2023diffusionnag,
  title={DiffusionNAG: Predictor-guided Neural Architecture Generation with Diffusion Models},
  author={An, Sohyun and Lee, Hayeon and Jo, Jaehyeong and Lee, Seanie and Hwang, Sung Ju},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{wang_mixture--experts_2023,
	title = {Mixture-of-{Experts} in {Prompt} {Optimization}},
	copyright = {CC0 1.0 Universal Public Domain Dedication},
	url = {https://openreview.net/forum?id=sDmjlpphdB},
	abstract = {Large Language Models (LLMs) exhibit strong generalization power in adapting to novel tasks when prompted with language instructions and in-context demos. Since this ability sensitively depends on the quality of prompts, various methods have been explored to automate the instruction design process. While these methods demonstrated promising results, they also restricted the output space of the search problem to a demo-free instruction. Such simplification significantly limits their performance, as a single demo-free instruction might not be able to cover the entire problem space of the targeted task due to its complexity. To alleviate this issue, we adopt the Mixture-of-Expert paradigm to divide the problem space into homogeneous regions, each governed by a specialized expert. To further improve the coverage of each expert, we expand their prompts to contain both an instruction and several demos. A two-phase process is developed to construct the specialized expert for each region: (1) demo assignment: Inspired by the theoretical connection between in-context learning and kernel regression, we group demos into clusters based on their semantic similarity and assign a cluster to each expert; (2) instruction assignment: A region-based joint search is applied to optimize an instruction complementary to the demo cluster for each expert, yielding a synergistic effect. The resulting method, codenamed Mixture-of-Prompts (MoP), outperforms prior art by up to 43\% on benchmark NLP tasks.},
	language = {en},
	urldate = {2024-02-27},
	author = {Wang, Ruochen and An, Sohyun and Cheng, Minhao and Zhou, Tianyi and Hwang, Sung Ju and Hsieh, Cho-Jui},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/Users/ansohyeon/Zotero/storage/JG8AYHEB/Wang 등 - 2023 - Mixture-of-Experts in Prompt Optimization.pdf:application/pdf},
}
